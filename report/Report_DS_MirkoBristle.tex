% TEMPLATE for Usenix papers, specifically to meet the requirements of
%  USENIX '05
% originally a template for producing IEEE-format articles using LaTeX.
%   written by Matthew Ward, CS Department, Worcester Polytechnic Institute.
% adapted by David Beazley for his excellent SWIG paper in Proceedings,
%   Tcl 96
% turned into a smartass generic template by De Clarke, with thanks to
%   both the above pioneers
% use at your own risk. Complaints to /dev/null.
% make it two columns with no page numbering, default is 10 point

% Munged by Fred Douglis <douglis@research.att.com> 10/97 to separate
% the .sty file from the LaTeX source template so that people can
% more easily include the .sty file into an existing document. Also
% changed to more closely follow the style guidelines as represented
% by the Word sample file.

% Note that since 2010, USENIX does not require endnotes. If you want
% foot of page notes, don't include the endnotes package in the
% usepackage command, below.

\documentclass[letterpaper, twocolumn,11pt]{article}
\usepackage{usenix, epsfig, endnotes}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{hhline}
\begin{document}

    \algdef{SE}[SUBALG]{Indent}{EndIndent}{}{\algorithmicend\ }%
    \algtext*{Indent}
    \algtext*{EndIndent}

%make title bold and 14 pt font (Latex default is non-bold, 16 pt)
    \title{\Large \bf Multivariate Time Series Forecasting with Graph Neural Networks \\
    \Medium \it A Report for the Data Science Seminar
    }

    \author{
            {\rm Mirko Bristle, 14-109-576}\\
        University of Fribourg \\
        \\
        \today
    }
    \maketitle

% Use the following at camera-ready time to suppress page numbers.
% Comment it out when you first submit the paper for review.
%\thispagestyle{empty}

%
%\subsection*{Abstract}
%Your Abstract Text Goes Here. Just a few facts.
%Whet our appetites.


    \section{Introduction}

    Prediction of time series has a strong history with nevertheless importance today. With the rise of IoT devices and a substantial
    increase in information gathering in various domains started. This laid the bases for the current state-of-the-art time series forecasting.
    The applicability of time series prediction can be found in several domains, such as finance and electricity markets, but also in several fields in science, such as climate \cite{mudelsee2019trend}, and medicine \cite{topol2019high}.

	With the rise of machine learning and increased availability of time series data in combination with computational power, the trend from parametric models such as autoregressive models \cite{box2015time} exponential smoothing \cite{gardner1985exponential} continued towards more data-driven approaches such as gaussian processes \cite{williams1995gaussian}, support vector regression \cite{smola2004tutorial}, and more recently a wide variety of deep learning methods \cite{lim2021time, torres2021deep}.

   Time series forecasting can thereby be separated into univariate and multivariate forecasting. While the former relates to forecasting a single variable, such as temperature, the latter tries to predict multiple variables together. Multivariate forecasting has a wide range of applications, such as modeling streams of tourists \cite{cankurt2016tourism},  over financial data \cite{torres2018applying}, to traffic prediction \cite{yin2016forecasting}.

This report investigates a recent paper by Wu et al. \cite{wu2020connecting}, which introduced a graph-based neural network for multivariate time series forecasting.

    \section{Methodical Background}
    The paper investigated in the current report can be placed in the methodical perspective of time series forecasting while, on the other hand taking advantage of the more recently developed toolbox of graph neural networks (GNN).

    \subsection{Multivariate Time series forecasting (MTSF)}
    While time series has a long history, only a few relevant techniques for this paper should be described in this section.

    \textbf{VAR:} Vector autoregression models were a generalization from univariate time series forecasting. They assume a linear dependency between all variables, which leads to a highly parameterized model with a quadratic growth concerning the number of dimensions of the time series \cite{lutkepohl2013vector}. Attempts have been made to tackle these problems, and the model has been proven, especially in finance forecasting \cite{lutkepohl2009econometric}.

    \textbf{GP} The gaussian process models are based on the structural property of bayesian neural networks and often use Markov chain Monte Carlo simulations. They are often used in a medical academic setting \cite{CHEN200759}. The VAR and the GP model are prone to overfitting due to the high number of tunable parameters \cite{wu2020connecting}.

    \textbf{RNN} Recurrent neural networks attempt to capture temporal patterns for long-term dependencies as well as for series of variable length by introducing so-called Gated Recurrent Units (GRU) \cite{che2018recurrent}.

    \textbf{LSTM}  Long-short-term models are variants of recurrent networks that introduce an attention mechanism. To mention two variants, TPA-LSTM introduces a set of filters to capture time-scale independent temporal patterns and use the so-captured frequency domain information for multivariate time series prediction \cite{shih2019temporal}. A second example is the LSTNet. Lai et al. \cite{lai2018modeling} used a convolution neural network and a recurrent neural network to extract long-term patterns and short-term dependencies between the variables. Both models use an implicit mechanism to model the pair-wise variable dependencies.

    \subsection{Graph neural networks}
Introducing graphs into multivariate time series prediction is not far-fetched. Graphs give the possibility to model highly complex relations between different nodes. They are used in many different domains and can easily be applied to the time series forecasting problem by referring to a time series variable as a node and capturing the spatial relations between the nodes in a graph representation.

Graph neural networks (GNN) have gained popularity in recent years by proving their expressive power in different machine learning tasks \cite{zhou2020graph}. While RNNs can capture cyclic patterns in data, the idea originates back to optimizing a state transition system until it converges \cite{lecun1998gradient}. GNNs are in that regard generalizations of conventional CNNs. GNNs overcome the restriction from being limited to euclidean space by introducing a structure that can handle non-Euclidean domains \cite{bronstein2017geometric}.

    \section{MTGNN: Connecting the Dots}
    In the paper of Wu et al. \cite{wu2020connecting}, two significant challenges concerning the combination of GNN and multivariate
    time series forecasting are addressed.
    The graph structure in MTSF represents the relation between the different temporal features.
    A major issue is that without prior knowledge, the relationship between these features is unknown.
    Further, the graph structure differs between different data sets, which makes it impossible to have a complete end-to-end framework that adjusts to the properties of the graph.
    The second challenge addresses the issue that a previously learned or explicitly supplied graph structure might not be optimal, and thus,
    should be updated during the learning process.
    This is a step most of the previously published GNNs leave aside while focusing only on message passing between the nodes.

    \subsection{Formulating the Problem}
    A $N$ dimensional multivariate time series consists of a series of steps in time $z_t \in R^N$, with $z_t[i] \in R$
    being the $i^{th}$ - variable at time step $t$. The objective is to predict either a Q-step-away value
    $Y = \{z_{t_{P+Q}}\}$ or a sequence of M values $Y = \{z_{t_{P+1}}, z_{t_{P+2}},\dots,z_{t_{P+M}}\}$
    from a past time series with $P$ steps $X = \{z_{t_{1}}, z_{t_{2}},\dots,z_{t_{P}}\}$.

    The P past steps  $\mathbb{X} = {S_{t_1}, S_{t_2}, \dots, S_{t_P}}$ can be supplemented by $D$ auxiliary features s.d. the $i_{th}$-step
    $S_{t_i} \in R^{N+D}$ has $z_t[i]$ as the first column, followed by the auxiliary features.

    The objective is to minimize the loss of the mapping $f: \chi \rightarrow Y$ with $L2$-regularization.

    Let $G$ be a graph consisting of nodes $V$ and edges $E$, then formally $G=(V, E)$. The vertice $v \in V$ can be
    interpreted as the $i^{th}$ - variable $z_t[i]$. Thus, the number of nodes in $G$ can be described with $N$.
    Further, $e = (u,v)$ represents an edge from node $u$ to node $v$, which represents the relation between the nodes.

    The Node Neighborhood of a node $v \in V$ can then be described as $N(v) = \{u \in V| (u,v) \in E\}$.
    The relationship of the nodes and, with that, the existence of edges between every two nodes $v_i,v_j \in G$
    can be described in a adjacency Matrix $A \in R^{N\times N}$ by
    having  $A_{ij} = c > 0$ if $(v_i,v_j) \in E$. If there is no edge $(v_i,v_j) \notin E$ we write $Aij = 0$.

    \subsection{Architectual overview}
    The authors propose three significant components to address the challenges. A new graph learning layer
    is introduced to attack the first challenge of the unknown graph structure. The goal is to extract a first proposal of the relations between the features. The model's output is a sparse adjacency matrix based on the supplied training data.
    Based on the graph learning layer, the spacial dependencies among the features are learned in a graph convolution module to address the over-smoothing problem.
    The temporal convolution module aims at catching temporal patterns with different frequencies and lengths should by a customized 1D convolution.

    \subsection{Graph Learning Layer}
    One encounters two major issues when trying to assess the relationships of different time series variables.
    First, suppose the graph $G$ is a directed graph and thus $(u,v)\in E$ might differ from $(v,u)\in E$,
    then calculating the full adjacency matrix by a common distance metric as the euclidean distance results in time and space complexity of $O(N^2)$.
    Forecasting a time series with many dimensions is nearly impossible with such an approach.

    Having a directed graph with a potentially asymmetric adjacency matrix is not far-fetched.
    In practice, having a node influenced by another node but not vice versa can easily be observed by the phenomenon of traffic flow.
    More generally, any signal flowing over a net of nodes over time will lead to uni-directed relations.
    Perpendicularly, most distance metrics lead to a symmetric adjacency matrix as they are mostly bi-directional.

    To address these issues, the authors propose a method to extract uni-directional relationships. Further they restrict the sampling
    to only a subset of nodes for each mini-batch to alleviate the time and space complexity.

    For two randomly initialized node embeddings $E_1, E2$, the uni-directional relation is calculated as

    \begin{alignat}{1}
        M_1 &= \tanh (\alpha E_1 \Theta_1) \\
        M_2 &= \tanh (\alpha E_2 \Theta_2)  \\
        A &= ReLU(\tanh (\alpha(M_1 M_2^T - M_2 M_1^T))) \label{eq:adjacency_matrix}
    \end{alignat}
    \begin{algorithm}
        \caption{Algorithm: argtopk}\label{alg:argtopk}
        \begin{algorithmic}[1]
            \For{n = 1,2, \dots, N}
                \State $idx \gets argtopk(A[n,:], 1)$
                \State $A[n, -idx] \gets 0$
            \EndFor
        \end{algorithmic}
    \end{algorithm}
    where the saturation rate is controlled by the hyper-parameter $\alpha$ and  $\Theta_i, \Theta_j$ represent
    model parameters. The adjacency matrix $A$ is regularized by the equation \ref{eq:adjacency_matrix}
    by setting the negative counterpart of a positive valued cell $A_{ij}$ to $A_{ji} = 0$. In the algorithm argtopk \ref{alg:argtopk}
    all weights apart from the top k are set to 0 for each node.
    This means we keep only the edges between the node and its top k neighbors and make the adjacency matrix thus sparse to improve the computation in the graph convolution module.

    This is an attempt to have a flexible way of learning stable and interpretable features during training and can still be updatable by new training data after the online training process.
    The authors refrained from updating the weights dynamically over time as this prevents the model from convergence.
    To benefit from external knowledge about the graph structure of the model, it is possible to add a static node feature matrix $Z$
    which is then used to calculate the adjacency matrix with $E_1 =  E_2 = Z $.

    \subsection{Graph Concolution Module}
    The authors tried to integrate information about the node neighbors by introducing two mix-hop layers in the so-called graph convolutional module.

    The key goal of the mix-hop layer is to integrate the information of different neighbors with different distances by compromising on time, and space complexity \cite{AbuElHaija}.
    More generally, the neighborhood aggregation in this step is a variant of Laplacian smoothing \cite{leskovec2005graphs}.
    A well-known difficulty is the potentially negative impact of information given by the nodes in higher levels of the signal stream, such as the root of the graph.
    This can be handled by keeping a proportion of the original hidden states while at the same time integrating the information
    of the node neighborhood.
    It is also necessary to avoid the multi-layer network reaching the random walk's limit distribution.
    If not handled correctly, the propagation along the graph structure converges towards a single point. This happens especially with an increasing number of graph convolution layers. \cite{klicpera2018predict}.

    The authors tackled these issues by introducing an improved version of the mix-propagation layer that functions in an
    information propagation step and an information selection step.
    The information propagation step defines the $i-th$ hop with $A$ being the adjacency matrix as
    \begin{alignat}{1}
        H^{(i)} &= \beta H_{in} + (1-\beta) \tilde{A} H^{(i-1)} \label{eq:hop}
    \end{alignat}
    where the hidden state input is set as $H^0 = H_{in}$, $\tilde{A} = \tilde{D}^{-1}(A +I)$ with $\tilde{D}_{ii}=1 + \sum_{j}^{K}A_{ij}$.
    The hyperparameter $\beta$ defines the ratio between the amount of new information flowing in by the next hop versus the information of the original hidden state input.

    In the second step, the information is selected by the parameter matrix $W^{i}$ as follows:

    \begin{equation}
        H_{out} =\sum_{i}^{K} H^i W^i  \label{eq:hop_selection}
    \end{equation}

    In the end, one of the two mix-hop layers are run, one with the adjacency matrix and the other with the transposed adjacency matrix.
    The output of the graph convolution module is then the sum of both mix-hop layers.

    \subsection{Temporal Convolution Module}
    In this step, the goal is to catch temporal information in a set of high-level features.
    Therefore the authors introduce a dilatated inception layer which is executed once with a tangent hyperbolic activation function at the end
    and a second time with a sigmoid activation function at the end.
    The goal of the first activation function is to act as a filtering mechanism, while the latter acts as a gating function
    to control the amount of information passed to the next module.
    The output of the module is the multiplied result of the two layers.

    The dilated inception layer is inspired by two strategies to do temporal 1D convolution.
Filtering with kernels of different sizes was used to capture temporal patterns of different lengths.
    Secondly, to capture long-time sequences, a dilated convolution strategy was used.
    This acts as if the kernel is extended, leaving out samples, and thus it can be sensitive to longer time sequences \cite{xi2018deep}.

    The first strategy was strongly inspired by image processing.
    Applied to the 1-dimensional temporal convolution problem, the authors identified the filtering sizes of 7, 12, 24, 24, 28, 60
    as the major lengths regarding cycling events in time series.
    All these cycles can be described by stacking multiple inception layers of size $1 \times 2$, $1 \times 3$, $1 \times 6$, and $1 \times 7$.

    The dilated convolution addresses the issue of the linear receptive field size depending on the number of convolutional layers $m$ and the kernel $c$.
    The receptive field describes the window of a time series input that can, as a maximum, be taken into account to extract temporal patterns.
    Without this strategy, the receptive field $R$ is determined by

    \begin{alignat}{1}
        R &= 1 + m(c-1)  \label{eq:receptive_field}
    \end{alignat}

    To have a big receptive field, Szegedy et al. \cite{Szegedy_2015_CVPR} points out that the computational budget poses a significant bottleneck.
    Inspired by this paper, the authors introduced a down-sampled version of the dilated convolution.
    A dilation factor $d$ refers to the number of samples skipped. E.g., $d=3$ refers to taking every third sample of the input in the convolution.
    By introducing an adaptive dilation factor, they can let the receptive field size grow exponentially concerning the number of hidden layers in the network.
    Formally, let's consider $q \in \mathbb{R}>1$ as the rate of the exponential growth of the dilation factor for each layer,
    the receptive field $R$ can be described as

    \begin{alignat}{1}
        R &= 1 + (c-1)(q^m-1)/(q-1).  \label{eq:adaptive_receptive_field}
    \end{alignat}


    As last step, the concatenation of the inception and dilation was performed such that for a sequence $z \in \mathbb{R}^T$ of length $T$ and the introduced filters
    $f_{1 \times 2} \in \mathbb{R}^2$,
    $f_{1 \times 3} \in \mathbb{R}^3$,
    $f_{1 \times 6} \in \mathbb{R}^6$,
    $f_{1 \times 7} \in \mathbb{R}^7$,
    the dilated inception layer is composed by

    \begin{alignat}{1}
        z &= concat(
        z \star f_{1 \times 2},
        z \star f_{1 \times 3},
        z \star f_{1 \times 6},
        z \star f_{1 \times 7}
        ).
        \label{eq:dilated_inception_layer_wrt_z}
    \end{alignat}

    Whereby a dilated convolution of size $k$ for t is

    \begin{equation}
        z \star f_{1 \times k}(t) = \sum_{s=0}^{k-1} f_{1 \times k}(s)z(t-d \times s)
    \end{equation}

    \subsection{About skip connections and the output module}
    The skip connection layers should avoid the gradient vanishing problem by introducing a layer after each temporal convolution module.
    Further, to address the same issue, residual connections were introduced to the layer that maps from before a temporal convolution module to the next temporal convolution module, skipping one graph convolution module.
    For the $i-th$ skip connection layer with input size $L_i$, we have  $1 \times L_i$ convolutions and standardizing output to the sequence size 1.
    The output is then a standard $ 1 \times 1$ convolution layer to reshape the channels from the inputs and the skipping layers to the desired output dimension.

    \subsection{The learning Algorithm}
    With an increased number of multivariate variables and such a big graph size,
    performance issues concerning runtime complexity and memory must be considered.
    When training the GNN, often all nodes must be held as in-memory representations, which might exceed the memory capacity for large graphs.

    Inspired by the sub-graph training algorithm of Chiang et al. \cite{chiang2019cluster}, the authors split the graph
    randomly for each training iteration.
    The nodes can then learn the resulting sub-graph.
    Over the learning process, all node combinations can possibly be learned and updated.
    With this, they can reduce the time and space complexity from $\mathbf{O}(N^2)$ to $\mathbf{O}\left((\frac{N}{s})^2\right)$ for the graph learning layer.
    After having well-trained node embeddings, the global graph structure can be reconstructed to take full advantage of spatial learning.

    \begin{algorithm}
        \scriptsize
        \caption{The proposed learning algorithm}\label{learning}
        \textbf{Input}

        \begin{alignat*}{1}
            \mathcal{O} &= \text{dataset} \\
            \mathcal{V} &= \text{node set} \\
            \Theta &= \text{random init. model parameters} \\
            f(\cdot) &= \text{ initialized MTGNN model with } \Theta  \\
            \gamma &= \text{learning rate} \\
            b &= \text{batch size} \\
            s &= \text{step size} \\
            m &= \text{split size(default $\gets$ 1)}
        \end{alignat*}

        \begin{algorithmic}[1]
            \State $iter \gets 1$
            \State $r \gets 1$

            \Repeat
                \State $\mathcal{X} \in \mathbb{R}^{b \times T \times N \times D},\mathcal{Y} \in \mathbb{R}^{b \times T' \times N} \gets \text{sampled from }\mathcal{O}$
                \State $W \gets \text{random split of V s.d.} \cup_{i=1}^m V_i = V $
                \If{$iter \% s = 0 \land r \leqslant T'$ }
                    \State $r \gets r+1$
                \EndIf

                \For{$i \in 1:m $}
                    \State $\hat{\mathcal{Y}} \gets predict(f, \mathcal{X}[:,:,i,:], \Theta)$
                    \State $\mathcal{L} \gets loss(\hat{\mathcal{Y}}[:,r,:], \mathcal{Y}[:,r,i])$
                    \State $\Theta \gets train\_sgd\_and\_update\_weights(\Theta,\mathcal{L}, \gamma)$
                \EndFor
            \Until{$convergence$}
        \end{algorithmic}
    \end{algorithm}

    Further, they tried to optimize the multi-step forecasting by accounting for the fact that near-by predictions usually have smaller losses than far-away predictions.
    Optimizing only on the overall loss causes thus a distorted image of the error.
    So they applied a curriculum learning strategy that only looks at the next step first and then enlarges the scope of the prediction.
    By this, the difficulty of predicting a point far in the future can be reduced.


    \section{Experimental evaluation}
    This section gives a brief overview of their evaluation of the MTGNN algorithm.
    The authors compared the MTGNN algorithm to a selection of other multivariate time series models for both single-step prediction and predicting a sequence of steps in the future.

    \subsection{single step}
    For the single-step prediction, the results were compared based on the root-squared error
    and the empirical correlation $r$.
    The benchmark datasets consisted out of
    one traffic dataset (862 nodes, 17544 samples),
    a solar-energy dataset (137 nodes, 52560 samples),
    an electricity dataset (321 nodes, 26304 samples),
    and an exchange-rate dataset (8 nodes, 7588 samples).
    For each dataset, the input length was fixed to 168 samples, and the output was one sample.
    The model was trained with a so-called fixed horizon, such that the predicted sample was either 3, 6, 12, or 24 samples in the future.
    Two versions of the MTGNN model were trained.
    One with graph sampling and one without.
    Both were compared against an
    auto-regressive model (AR),
    a model consisting of a multi-layer perceptron combined with an AR called VAR-MLP,
    a gaussian-process model (GP),
    a recurrent neural network (RNN-GRU),
    a long-short-term time series neural network (LSTnet),
    and deep learning model called TPA-LSTM, which introduces a temporal pattern attention mechanism.

    Looking at the results, we can observe that for the traffic and the electricity dataset, the MTGNN model had the best performance concerning RSE and CORR for a horizon of 3, 12, 24 samples.
    The MTGNN model with sampling showed the lowest RSE and the highest correlation for a horizon of 6.
    For the solar-energy dataset, the MTGNN model was only excelled by the TPA-LSTM model for the RSME and the CORR with a horizon of 6 and analoge for CORR with a horizon of 24.
    For the exchange rate, the TPA-LSTM showed the best performance for all horizons concerning the RSE and the CORR, both with a horizon of 3.
    For a horizon of 12 and 24, the sampled MTGNN showed the highest CORR, while for a horizon of 6, the RNN-GRU outperformed the other models with regard to the CORR.

    \subsection{multistep-prediction}
    For the comparison of the multi-step prediction, the mean absolute error (MAE), the root mean square error (RMSE), and the mean absolute percentage error (MAPE) was used as a comparison metric.
    Two spatial-temporal graph neural network datasets were used for model benchmarking and compared against six other graph neural networks.
    Of relevance is the WaveNet, a network that integrates diffusion graph convolutions with 1D dilated convolutions,
    MRA-BGCN, which is a multi-range attentive bicomponent graph neural network, and
    GMAN is a graph neural network with spatial and temporal attention mechanisms.
    The datasets were the METR-LA dataset with 207 nodes and 34272 samples and the PEMS-BAY dataset with 325 nodes and 52116 samples.
    The input size to all networks was 12 samples, and the output was also set to 12 samples.
    The evaluated horizons were 3, 6, and 12.

    The best performing model for the METR-LA dataset for a horizon of 3 was with regards to MAE and RMSE in the MRA\_BGCN model, while the MTGNN sampling model showed the lowes MAPE.
    The MGTNN model performed best in all metrics for a horizon of 6, and the GMAN model performed best with regard to all metrics for a horizon of 12 steps.
    For the PEMS-BAY dataset, GMAN showed in all metrics the best performance for a horizon of 12 steps, while MRA-BGCNM showed the best performance for a horizon of 3 and 6 for the MAE and the RMSE.
    The MRA-BGCNM also showed the best performance for the MAPE with a horizon of 6, and the WaveNet model had the lowest MAPE for a horizon of 3.

    \subsection{The ablation study}
    In order to remove the effectiveness of the components, an ablation study was conducted, whereby the fully trained model was compared to a version without each of the four variations.
    First, the graph convolution module was removed and substituted by a linear layer.
    Secondly, the mix-hop propagation layer, where the neighborhood was integrated, was bypassed.
    Third, the inception in the dilated inception layer was replaced by a single 1 x 7 filter.
    Fourth, the curriculum learning was removed, and thus, the model was not trained step-wise.
    Each of the models was trained 10 times and with 50 epochs each.
    The mean and the standard deviation of the MAE, RMSE, and MAPE were then reported.

    For all of the metrics, the full MTGNN model showed the best performance.
    The authors claim that the model improves significantly by the graph convolution layer and thus makes use of the information flow among the nodes.
    The mix-hop layer appears to contribute to the information selection in each information propagation step.
    Further, they claim that the inception contributes significantly to the model for the RMSE (although only marginal for MAE).
    Also, the curriculum learning strategy is claimed to be effective by first converging on an easy task and then step-wise calibrating the parameters.

    \subsection{graph learning layer}
    The evaluation of the graph learning layer was performed on the METR-LA data only and evaluated on the validation set with 10 runs.
    Their method of constructing the adjacency matrix $A$ called the uni-directed-A was compared to a predefined A, a global - A with $n^2$ parameters, an undirected-A and directed-A representing the similarities of the node embeddings, and at last a dynamic - A which takes the temporal change of the nodes into account.
    Based on the MAE, MAPE, and RMSE, the uni-directed method performed significantly better than the other methods.


    \section{Report Experiments}
    In this section, some of the results should be replicated, and additional experiments will be conducted to get a more concise understanding of the MTGNN network.
    Due to limited computational power, only a very little amount could be carried out.

    \subsection{Methods}
    The primary focus lies in replicating the reported results.
    To accomplish this, the same experiments as described above should be executed.
    Especially of interest is the validation of the single-step and multi-step forecast in order.
    Although the authors supplied the mean of the experiments, they missed out on the standard deviation or any disperson measure.

    Ideally, for interpreting the results, one needs to rerun the experiments at least the proposed 10 times to see if the increase of MTGNN is significant at all and with what effect size.
    Assuming that the residuals are uniformly distributed, the remaining models could then easily be compared against the MTGNN model. As equal variances between the models can be expected, a pooled-variance t-test is should be used.

    The same procedure should then be carried out for the ablation study.

    \subsection{Results}
    Due to computational complexity in time and space, only four samples were yet calculated.


    \textbf{Single-step learning} The calculated samples include one for each of the four single-step forecasting datasets.
    Due to computational resource limitations, the number of epochs varied from the 30 epochs used in the original experiment.
    Nonetheless, the reader can get a first impression of the reproducibility of the results in Table \ref{tab:table}.

   \begin{table}
       \centering
       \caption{Overview of the replication experiments}
       \scriptsize
       \begin{tabular}{lllll}
           \hline
           \textbf{Experiment / Dataset} & \textbf{RSE} & \textbf{CORR} & \textbf{epochs} & \textbf{horizon} \\
           \hhline{=====}
           org Solar-energy~             & 0.1778       & 0.9852        & 30              & 3                \\
%            \hline
           rep. Solar-energy             & 0.1821       & 0.9845        & 15              & 3                \\
           \hhline{-----}
           org. Electricity              & 0.0745       & 0.9474        & 30              & 3                \\
%            \hline
           rep. Electricity              & 0.0780       & 0.943         & 10               & 3                \\
           \hhline{-----}
           org. Traffic                  & 0.4162       & 0.8963        & 30              & 3                \\
%            \hline
           rep.Traffic                   &   0.436        &  0.887       & 10               & 3                \\
           \hhline{-----}
           org. Exchange                 & 0.0194       & 0.9786        & 30              & 3                \\
%            \hline
           rep. Exchange                 & 0.0191       & 0.9795        & 30              & 3                \\
           \hline
       \end{tabular}\label{tab:table}
   \end{table}

    \textbf{Multi-step learning}. This section is still missing as the code was not rewritten and run. Thus, no results have been obtained.

    \textbf{Ablation Study}. The models for this section were not retrained. Since the mean and the standard deviation were presented, the significance estimates could be recalculated.
\begin{table}
       \scriptsize
\centering
\caption{Ablationstudy significance test}
\begin{tabular}{llll}
              & MAE                                                   \\
w/o GC        & p=7.907e-16,t=-26.3452, $r^2=11.78$~ (large)  \\
\hhline{--}\\
w/o Mix-Hop   & p=2.974e-5,t=-5.5329, $r^2=2.47$~ (large)     \\
\hhline{--}\\
w/o Inception & p=0.2614,t=-1.1596, $r^2=0.52$~ (medium)      \\
\hhline{--}\\
w/o CL        & p=0.03707,t=-2.2516, $r^2=1.01$~ (large)      \\
\hhline{==}\\
              & RSME                                                                                          \\
w/o GC & p=2.562e-12, t=-16.5103,$r^2=7.38$ (large)      \\
\hhline{--}\\
w/o Inception & p=0.04355, t=-2.171,$r^2=0.97$ (large)                                           \\
\hhline{--}\\
w/o Inception & p=0.4028, t=-0.857,$r^2=0.38$ (medium)                                           \\
\hhline{--} \\
w/o CL        & p=0.3829, t=-0.8944,$r^2=0.4$ (medium)                                             \\
\hhline{==}\\
              & MAPE                                              \\
w/o GC        & p=1.116e-10,t=-13.168, $r^2=5.89$~ (large)\\
\hhline{--}\\
w/o Mix-Hop   & p=0.8066,t=-0.2485, $r^2=0.11$~ (small)   \\
\hhline{--}\\
w/o Inception & p=1,t=0, $r^2=0$~ (small)               \\
\hhline{--}\\
w/o CL        & p=0.1534,t=-1.4907, $r^2=0.67$~ (large)   \\
\hhline{==}
\end{tabular}
\end{table}
As expected, the network without the graph convolution module was significantly lower than the full MTGNN for all metrics and showed a large effect size.
The model without the Mix-Hop layer performed significantly worse with regards to MAE and RSME as effect sizes, while the effect was only small and not significant for the MAPE.
Surprisingly, the MTGNN did not perform significantly better than without the inception layer. The effect sizes were medium for MAE and RSME and small for MAPE.
While MTGNN performed with a large effect size significantly better for the MAE, it did not matter when looking at the RSME (effect size medium) or the MAPE (effect size large).
All calculations had equal variances when compared in a two-tailed F-Test.

    \section{Discussion}
	The study of Wu et al. introduces an interesting new framework for time series forecasting by combining state-of-the-art methods of graph neural networks and time series prediction.
    Their result shows that the MTGNN can combat, if not even outperform, leading forecasting methods for single-step forecasting. By looking at the properties, we can speculate that their network is especially powerful for time series data that has high spacial inter-dependency with a rather high number of samples and nodes.
    As the space complexity also depends on the number of nodes,  the introduced sub-graph learning increases the model's power even more.
   Further, for multi-step forecasting, the model does perform almost equally well as the reference graph learning models chosen. Although, the performance does differ depending on the benchmark dataset.

    \subsection{Integreation of Evaluation and Replication}
The performed replication experiment was up now not entirely conducted. Thus, only preliminary conclusions can be drawn.
Looking at the single-step prediction, although the model was not trained with the same number of epochs for the solar, electricity, and traffic dataset, we get for all three datasets near equal performance with one-third, respectivly half of the training epochs.
The TPA-LSTM model and the MTGNN implementations were the only ones that surpassed the achieved result for solar data. The replicated traffic model outperformed all other models apart from the MTGNN, and the same could be observed for the electricity model.
We even surpassed the performance on both evaluation metrics for the exchange dataset, which was trained on the same number of epochs as the original experiment. Comparing the replicated model to the other models, we rank the RSE now second best, only surpassed by the TPA-LSTM model. The correlation of the exchange model ranks first with better performance than any other model.
As all the experiments were only run once and no measure of variation was given, this ordinal comparison can not further evaluate the observed values in the study by the authors. Nevertheless, the results show the strong performance of the models.

The statistical evaluation of the ablation study was somewhat surprising, as it partially contrasts with the authors' interpretation.
A rather obvious explanation for the high rate of not significant findings is, on the one hand, the relatively low sample size. Due to this, it might very likely be that the small and medium effects were not detected.
The graph convolution shows the most substantial significant effect. This indicates that the model benefits from the information flow between the different nodes and that the structural information contributes to the classification result.
The Mix-hop layer with such a part of the graph convolution layer, at least regarding the MAE and RSME, is beneficial for the model. This means the neighborhood information gathered and passed along the nodes contributes to the forecasting results.
Only medium to small non-significant effects were shown for the inception layer. As the authors pointed out, this might be due to the increased parameters due to the replaced 1x7 filter. Thus, one can question the informative value of the test itself.
The curriculum learning showed a somewhat mixed result. This can mean that the incremental learning strategy did not work as well as expected or that the effect shows more in one metric than another. Due to the small sample size, it is rather hard to make a clear call.

    \subsection{Critique}
    The study combines novel techniques and opens a new field in time series forecasting.
    While the benchmarking datasets cover a range of applications, no synthesized data with specific properties was used for evaluation.
    By this, the strength and weaknesses of the model could be much better evaluated.
    Interesting would have also been to investigate the choice of parameters more in depth.
    To the reader remains somewhat unclear why specific parameters were changed between the single-step and the multi-step forecasting.
    The same is true for the parameter choice of the exchange dataset.

    As mentioned above, the absence of accurate statistics in the model evaluation makes it even harder to interpret.


    \subsection{Future research}
    The field of graph neural networks about time series forecasting is still relatively young and provides opportunities for innovation.
    One such opportunity lies in identifying the properties of specific real-world datasets and adjusting the ideas presented in this paper to make full benefit in that regard.
    Further, it remains somewhat unclear how the model behaves in terms of robustness, missing values, and other distortions. Investigating such behaviors is still to be done.

    As mentioned, one limiting factor for big graphs is space and time complexity. Finding even more efficient graph embeddings and sampling methods could support bigger datasets and lead to faster computation.



    {\footnotesize \bibliographystyle{acm}
    \bibliography{bibliography}}


%     \theendnotess


   \section{Appendix}

   \subsection{Model parameters}\label{model_parameters}
   The same parameters as in the original study were used.

   Specifically for single-step forecasting, an Adam optimizer with a gradient clip of 5 was used.
   The learning rate was set to 0.001, and an L2 regularization penalty was applied.
   After each temporal convolution layer, a dropout of 0.3 was applied.
   The mix-hop layer had a depth of 2 with a retain ratio of 0.05.
   The saturation rate of the graph learning layer was 3 with 40 node embeddings.
   The whole network had 5 layers, each a graph learning and a temporal learning module.
   The dilated inception layer had an exponential dilation factor of 2.
   The first 1x1 convolution took 1 input channel with 16 output channels.
   The skip layers had 32 output channels.
   While the first layer of the output module had 64 channels, the second layer of the output module had only 1.
   The three datasets, Traffic, Solar, and Electricity, had for each node 20 neighbors, whereas the exchange rate had only 8.
   All datasets had a batch size of 4.
   Sampling was for simplicity not used and thus set to 1.
   As in the paper, the reported metric was the RSE and CORR.
\end{document}
